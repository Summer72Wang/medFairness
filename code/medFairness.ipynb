{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e32ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#from datasets.BaseDataset import BaseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56f86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, path_to_images, sens_name, sens_classes, transform):\n",
    "        super(BaseDataset, self).__init__()\n",
    "        \n",
    "        self.dataframe = dataframe        \n",
    "        self.dataset_size = self.dataframe.shape[0]\n",
    "        self.transform = transform\n",
    "        self.path_to_images = path_to_images\n",
    "        self.sens_name = sens_name\n",
    "        self.sens_classes = sens_classes\n",
    "        \n",
    "        self.A = None\n",
    "        self.Y = None\n",
    "        self.AY_proportion = None\n",
    "        \n",
    "    def get_AY_proportions(self):\n",
    "        if self.AY_proportion:\n",
    "            return self.AY_proportion\n",
    "        \n",
    "        A_num_class = 2\n",
    "        Y_num_class = 2\n",
    "        A_label = self.A\n",
    "        Y_label = self.Y\n",
    "        \n",
    "        A = self.A.tolist()\n",
    "        Y = self.Y.tolist()\n",
    "        ttl = len(A)\n",
    "            \n",
    "        len_A0Y0 = len([ay for ay in zip(A, Y) if ay == (0, 0)])\n",
    "        len_A0Y1 = len([ay for ay in zip(A, Y) if ay == (0, 1)])\n",
    "        len_A1Y0 = len([ay for ay in zip(A, Y) if ay == (1, 0)])\n",
    "        len_A1Y1 = len([ay for ay in zip(A, Y) if ay == (1, 1)])\n",
    "\n",
    "        assert (\n",
    "            len_A0Y0 + len_A0Y1 + len_A1Y0 + len_A1Y1\n",
    "        ) == ttl, \"Problem computing train set AY proportion.\"\n",
    "        A0Y0 = len_A0Y0 / ttl\n",
    "        A0Y1 = len_A0Y1 / ttl\n",
    "        A1Y0 = len_A1Y0 / ttl\n",
    "        A1Y1 = len_A1Y1 / ttl\n",
    "        \n",
    "        self.AY_proportion = [[A0Y0, A0Y1], [A1Y0, A1Y1]]\n",
    "        \n",
    "        return self.AY_proportion\n",
    "    \n",
    "    def get_A_proportions(self):\n",
    "        AY = self.get_AY_proportions()\n",
    "        ret = [AY[0][0] + AY[0][1], AY[1][0] + AY[1][1]]\n",
    "        np.testing.assert_almost_equal(np.sum(ret), 1.0)\n",
    "        return ret\n",
    "\n",
    "    def get_Y_proportions(self):\n",
    "        AY = self.get_AY_proportions()\n",
    "        ret = [AY[0][0] + AY[1][0], AY[0][1] + AY[1][1]]\n",
    "        np.testing.assert_almost_equal(np.sum(ret), 1.0)\n",
    "        return ret\n",
    "\n",
    "    def set_A(self, sens_name):\n",
    "        if sens_name == 'Sex':\n",
    "            A = np.asarray(self.dataframe['Sex'].values != 'M').astype('float')\n",
    "        elif sens_name == 'Age':\n",
    "            A = np.asarray(self.dataframe['Age_binary'].values.astype('int') == 1).astype('float')\n",
    "        elif sens_name == 'Race':\n",
    "            A = np.asarray(self.dataframe['Race'].values == 'White').astype('float')\n",
    "        elif self.sens_name == 'skin_type':\n",
    "            A = np.asarray(self.dataframe['skin_binary'].values != 0).astype('float')\n",
    "        elif self.sens_name == 'Insurance':\n",
    "            self.A = np.asarray(self.dataframe['Insurance_binary'].values != 0).astype('float')\n",
    "        else:\n",
    "            raise ValueError(\"Does not contain {}\".format(self.sens_name))\n",
    "        return A\n",
    "\n",
    "    def get_weights(self, resample_which):\n",
    "        sens_attr, group_num = self.group_counts(resample_which)\n",
    "        group_weights = [1/x.item() for x in group_num]\n",
    "        sample_weights = [group_weights[int(i)] for i in sens_attr]\n",
    "        return sample_weights\n",
    "    \n",
    "    def group_counts(self, resample_which = 'group'):\n",
    "        if resample_which == 'group' or resample_which == 'balanced':\n",
    "            if self.sens_name == 'Sex':\n",
    "                mapping = {'M': 0, 'F': 1}\n",
    "                groups = self.dataframe['Sex'].values\n",
    "                group_array = [*map(mapping.get, groups)]\n",
    "                \n",
    "            elif self.sens_name == 'Age':\n",
    "                if self.sens_classes == 2:\n",
    "                    groups = self.dataframe['Age_binary'].values\n",
    "                elif self.sens_classes == 5:\n",
    "                    groups = self.dataframe['Age_multi'].values\n",
    "                elif self.sens_classes == 4:\n",
    "                    groups = self.dataframe['Age_multi4'].values.astype('int')\n",
    "                group_array = groups.tolist()\n",
    "                \n",
    "            elif self.sens_name == 'Race':\n",
    "                mapping = {'White': 0, 'non-White': 1}\n",
    "                groups = self.dataframe['Race'].values\n",
    "                group_array = [*map(mapping.get, groups)]\n",
    "            elif self.sens_name == 'skin_type':\n",
    "                if self.sens_classes == 2:\n",
    "                    groups = self.dataframe['skin_binary'].values\n",
    "                elif self.sens_classes == 6:\n",
    "                    groups = self.dataframe['skin_type'].values\n",
    "                group_array = groups.tolist()\n",
    "            elif self.sens_name == 'Insurance':\n",
    "                if self.sens_classes == 2:\n",
    "                    groups = self.dataframe['Insurance_binary'].values\n",
    "                elif self.sens_classes == 5:\n",
    "                    groups = self.dataframe['Insurance'].values\n",
    "                group_array = groups.tolist()\n",
    "            else:\n",
    "                raise ValueError(\"sensitive attribute does not defined in BaseDataset\")\n",
    "            \n",
    "            if resample_which == 'balanced':\n",
    "                #get class\n",
    "                labels = self.Y.tolist()\n",
    "                num_labels = len(set(labels))\n",
    "                num_groups = len(set(group_array))\n",
    "                \n",
    "                group_array = (np.asarray(group_array) * num_labels + np.asarray(labels)).tolist()\n",
    "                \n",
    "        elif resample_which == 'class':\n",
    "            group_array = self.Y.tolist()\n",
    "            num_labels = len(set(group_array))\n",
    "        \n",
    "        self._group_array = torch.LongTensor(group_array)\n",
    "        if resample_which == 'group':\n",
    "            self._group_counts = (torch.arange(self.sens_classes).unsqueeze(1)==self._group_array).sum(1).float()\n",
    "        elif resample_which == 'balanced':\n",
    "            self._group_counts = (torch.arange(num_labels * num_groups).unsqueeze(1)==self._group_array).sum(1).float()\n",
    "        elif resample_which == 'class':\n",
    "            self._group_counts = (torch.arange(num_labels).unsqueeze(1)==self._group_array).sum(1).float()\n",
    "        return group_array, self._group_counts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "    \n",
    "    def get_labels(self): \n",
    "        # for sensitive attribute imbalance\n",
    "        if self.sens_classes == 2:\n",
    "            return self.A\n",
    "        elif self.sens_classes == 5:\n",
    "            return self.dataframe['Age_multi'].values.tolist()\n",
    "        elif self.sens_classes == 4:\n",
    "            return self.dataframe['Age_multi4'].values.tolist()\n",
    "\n",
    "    def get_sensitive(self, sens_name, sens_classes, item):\n",
    "        if sens_name == 'Sex':\n",
    "            if item['Sex'] == 'M':\n",
    "                sensitive = 0\n",
    "            else:\n",
    "                sensitive = 1\n",
    "        elif sens_name == 'Age':\n",
    "            if sens_classes == 2:\n",
    "                sensitive = int(item['Age_binary'])\n",
    "            elif sens_classes == 5:\n",
    "                sensitive = int(item['Age_multi'])\n",
    "            elif sens_classes == 4:\n",
    "                sensitive = int(item['Age_multi4'])\n",
    "        elif sens_name == 'Race':\n",
    "            if item['Race'] == 'White':\n",
    "                sensitive = 0\n",
    "            else:\n",
    "                sensitive = 1\n",
    "        elif sens_name == 'skin_type':\n",
    "            if sens_classes == 2:\n",
    "                sensitive = int(item['skin_binary'])\n",
    "            else:\n",
    "                sensitive = int(item['skin_type'])\n",
    "        elif self.sens_name == 'Insurance':\n",
    "            if self.sens_classes == 2:\n",
    "                sensitive = int(item['Insurance_binary'])\n",
    "            elif self.sens_classes == 5:\n",
    "                sensitive = int(item['Insurance'])\n",
    "        else:\n",
    "            raise ValueError('Please check the sensitive attributes.')\n",
    "        return sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import random\n",
    "import torchio as tio\n",
    "from utils.spatial_transforms import ToTensor\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    NormalizeVideo,\n",
    ")\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "def get_dataset(opt):\n",
    "    data_setting = opt['data_setting']\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "    if opt['is_3d']:\n",
    "        mean_3d = [0.45, 0.45, 0.45]\n",
    "        std_3d = [0.225, 0.225, 0.225]\n",
    "        sizes = {'ADNI': (192, 192, 128), 'ADNI3T': (192, 192, 128), 'OCT': (192, 192, 96), 'COVID_CT_MD': (224, 224, 80)}\n",
    "        if data_setting['augment']:\n",
    "            transform_train = transforms.Compose([\n",
    "                tio.transforms.RandomFlip(),\n",
    "                tio.transforms.RandomAffine((-15, 15)),\n",
    "                tio.transforms.CropOrPad(sizes[opt['dataset_name']]),\n",
    "                \n",
    "                ToTensor(),\n",
    "                NormalizeVideo(mean_3d, std_3d),\n",
    "            ])\n",
    "        else:\n",
    "            transform_train = transforms.Compose([\n",
    "                tio.transforms.CropOrPad(sizes[opt['dataset_name']]),\n",
    "                ToTensor(),\n",
    "                NormalizeVideo(mean_3d, std_3d),\n",
    "            ])\n",
    "    \n",
    "        transform_test = transforms.Compose([\n",
    "            tio.transforms.CropOrPad(sizes[opt['dataset_name']]),\n",
    "            ToTensor(),\n",
    "            NormalizeVideo(mean_3d, std_3d),\n",
    "        ])\n",
    "    elif opt['is_tabular']:\n",
    "        pass\n",
    "    else:\n",
    "        if data_setting['augment']:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation((-15, 15)),\n",
    "                transforms.RandomCrop((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "        else:\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "    \n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(opt['random_seed'])\n",
    "    def seed_worker(worker_id):\n",
    "        np.random.seed(opt['random_seed'] )\n",
    "        random.seed(opt['random_seed'])\n",
    "        \n",
    "    image_path = data_setting['image_feature_path']\n",
    "    train_meta = pd.read_csv(data_setting['train_meta_path']) \n",
    "    val_meta = pd.read_csv(data_setting['val_meta_path'])\n",
    "    test_meta = pd.read_csv(data_setting['test_meta_path'])   \n",
    "    \n",
    "    if opt['bianry_train_multi_test'] == -1:\n",
    "        val_test_classes = opt['sens_classes']\n",
    "    else:\n",
    "        val_test_classes = opt['bianry_train_multi_test']\n",
    "    \n",
    "    if opt['is_3d']:\n",
    "        dataset_name = getattr(datasets, opt['dataset_name'])\n",
    "        train_data = dataset_name(train_meta, image_path, opt['sensitive_name'], opt['sens_classes'], transform_train)\n",
    "        val_data = dataset_name(val_meta, image_path, opt['sensitive_name'], val_test_classes, transform_test)\n",
    "        test_data = dataset_name(test_meta, image_path, opt['sensitive_name'], val_test_classes, transform_test)\n",
    "    elif opt['is_tabular']:\n",
    "        # different format\n",
    "        dataset_name = getattr(datasets, opt['dataset_name'])\n",
    "        data_train_path = data_setting['data_train_path']\n",
    "        data_val_path = data_setting['data_val_path']\n",
    "        data_test_path = data_setting['data_test_path']\n",
    "        \n",
    "        data_train_df = pd.read_csv(data_train_path)\n",
    "        data_val_df = pd.read_csv(data_val_path)\n",
    "        data_test_df = pd.read_csv(data_test_path)\n",
    "        \n",
    "        train_data = dataset_name(train_meta, data_train_df, opt['sensitive_name'], opt['sens_classes'], None)\n",
    "        val_data = dataset_name(val_meta, data_val_df, opt['sensitive_name'], val_test_classes, None)\n",
    "        test_data = dataset_name(test_meta, data_test_df, opt['sensitive_name'], val_test_classes, None)\n",
    "    \n",
    "    else:\n",
    "        dataset_name = getattr(datasets, opt['dataset_name'])\n",
    "        pickle_train_path = data_setting['pickle_train_path']\n",
    "        pickle_val_path = data_setting['pickle_val_path']\n",
    "        pickle_test_path = data_setting['pickle_test_path']\n",
    "        train_data = dataset_name(train_meta, pickle_train_path, opt['sensitive_name'], opt['sens_classes'], transform_train)\n",
    "        val_data = dataset_name(val_meta, pickle_val_path, opt['sensitive_name'], val_test_classes, transform_test)\n",
    "        test_data = dataset_name(test_meta, pickle_test_path, opt['sensitive_name'], val_test_classes, transform_test)\n",
    "    \n",
    "    print('loaded dataset ', opt['dataset_name'])\n",
    "        \n",
    "    if opt['experiment']=='resampling' or opt['experiment']=='GroupDRO' or opt['experiment']=='resamplingSWAD':\n",
    "        weights = train_data.get_weights(resample_which = opt['resample_which'])\n",
    "        sampler = WeightedRandomSampler(weights, len(weights), replacement=True, generator = g)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                            train_data, batch_size=opt['batch_size'], \n",
    "                            sampler=sampler,\n",
    "                            shuffle=(opt['experiment']!='resampling' and opt['experiment']!='GroupDRO' and opt['experiment']!='resamplingSWAD'), num_workers=8, \n",
    "                            worker_init_fn=seed_worker, generator=g, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "                          val_data, batch_size=opt['batch_size'],\n",
    "                          shuffle=True, num_workers=8, worker_init_fn=seed_worker, generator=g, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "                           test_data, batch_size=opt['batch_size'],\n",
    "                           shuffle=True, num_workers=8, worker_init_fn=seed_worker, generator=g, pin_memory=True)\n",
    "\n",
    "    return train_data, val_data, test_data, train_loader, val_loader, test_loader, val_meta, test_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d71d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAPILA(BaseDataset):\n",
    "    def __init__(self, dataframe, path_to_pickles, sens_name, sens_classes, transform):\n",
    "        super(PAPILA, self).__init__(dataframe, path_to_pickles, sens_name, sens_classes, transform)\n",
    "\n",
    "        with open(path_to_pickles, 'rb') as f: \n",
    "            self.tol_images = pickle.load(f)\n",
    "            \n",
    "        self.A = self.set_A(sens_name) \n",
    "        self.Y = (np.asarray(self.dataframe['Diagnosis'].values) > 0).astype('float')\n",
    "        self.AY_proportion = None\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        img = Image.fromarray(self.tol_images[idx])\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label = torch.FloatTensor([item['Diagnosis']])\n",
    "        \n",
    "        sensitive = self.get_sensitive(self.sens_name, self.sens_classes, item)\n",
    "                \n",
    "        return idx, img, label, sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062874a",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d66df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cee3c0",
   "metadata": {},
   "source": [
    "## Preprocess metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata\n",
    "path = 'data/PAPILA/'\n",
    "\n",
    "demo_data = pd.read_csv(path + 'HAM10000_metadata.csv')\n",
    "demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(demo_data['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add image path to the metadata\n",
    "pathlist = demo_data['image_id'].values.tolist()\n",
    "paths = ['HAM10000_images/' + i + '.jpg' for i in pathlist]\n",
    "demo_data['Path'] = paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe91900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove age/sex == null \n",
    "demo_data = demo_data[~demo_data['age'].isnull()]\n",
    "demo_data = demo_data[~demo_data['sex'].isnull()]\n",
    "demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the value of sensitive attributes\n",
    "sex = demo_data['sex'].values\n",
    "sex[sex == 'male'] = 'M'\n",
    "sex[sex == 'female'] = 'F'\n",
    "demo_data['Sex'] = sex\n",
    "demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split subjects to different age groups\n",
    "demo_data['Age_multi'] = demo_data['age'].values.astype('int')\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(-1,19), 0, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(20,39), 1, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(40,59), 2, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi'].between(60,79), 3, demo_data['Age_multi'])\n",
    "demo_data['Age_multi'] = np.where(demo_data['Age_multi']>=80, 4, demo_data['Age_multi'])\n",
    "\n",
    "demo_data['Age_binary'] = demo_data['age'].values.astype('int')\n",
    "demo_data['Age_binary'] = np.where(demo_data['Age_binary'].between(-1, 60), 0, demo_data['Age_binary'])\n",
    "demo_data['Age_binary'] = np.where(demo_data['Age_binary']>= 60, 1, demo_data['Age_binary'])\n",
    "demo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to binary labels\n",
    "# benign: bcc, bkl, dermatofibroma, nv, vasc\n",
    "# maglinant: akiec, mel\n",
    "\n",
    "labels = demo_data['dx'].values.copy()\n",
    "labels[labels == 'akiec'] = '1'\n",
    "labels[labels == 'mel'] = '1'\n",
    "labels[labels != '1'] = '0'\n",
    "\n",
    "labels = labels.astype('int')\n",
    "\n",
    "demo_data['binaryLabel'] = labels\n",
    "demo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2660b",
   "metadata": {},
   "source": [
    "## split dataset to train/test/varify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_811(all_meta, patient_ids):\n",
    "    sub_train, sub_val_test = train_test_split(patient_ids, test_size=0.2, random_state=0)\n",
    "    sub_val, sub_test = train_test_split(sub_val_test, test_size=0.5, random_state=0)\n",
    "    train_meta = all_meta[all_meta.lesion_id.isin(sub_train)]\n",
    "    val_meta = all_meta[all_meta.lesion_id.isin(sub_val)]\n",
    "    test_meta = all_meta[all_meta.lesion_id.isin(sub_test)]\n",
    "    return train_meta, val_meta, test_meta\n",
    "\n",
    "sub_train, sub_val, sub_test = split_811(demo_data, np.unique(demo_data['lesion_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train.to_csv('your_path/fariness_data/HAM10000/split/new_train.csv')\n",
    "sub_val.to_csv('your_path/fariness_data/HAM10000/split/new_val.csv')\n",
    "sub_test.to_csv('your_path/fariness_data/HAM10000/split/new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can have a look of some examples here\n",
    "img = cv2.imread('your_path/fariness_data/HAM10000/HAM10000_images/ISIC_0027419.jpg')\n",
    "print(img.shape)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d22d2",
   "metadata": {},
   "source": [
    "## Save images into pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97985a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta = pd.read_csv('your_path/fariness_data/HAM10000/split/new_train.csv')\n",
    "\n",
    "path = 'your_path/fariness_data/HAM10000/pkls/'\n",
    "images = []\n",
    "start = time.time()\n",
    "for i in range(len(test_meta)):\n",
    "\n",
    "    img = cv2.imread(path + test_meta.iloc[i]['Path'])\n",
    "    # resize to the input size in advance to save time during training\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    images.append(img)\n",
    "    \n",
    "end = time.time()\n",
    "end-start\n",
    "with open(path + 'train_images.pkl', 'wb') as f:\n",
    "    pickle.dump(images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57573b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b5f43d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'parse_args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mparse_args\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'parse_args'"
     ]
    }
   ],
   "source": [
    "import parse_args\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import basics\n",
    "import glob\n",
    "\n",
    "\n",
    "def train(model, opt):\n",
    "    for epoch in range(opt['total_epochs']):\n",
    "        ifbreak = model.train(epoch)\n",
    "        if ifbreak:\n",
    "            break\n",
    "     \n",
    "    # record val metrics for hyperparameter selection\n",
    "    pred_df = model.record_val()\n",
    "    return pred_df\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    opt, wandb = parse_args.collect_args()\n",
    "    if not opt['test_mode']:\n",
    "        \n",
    "        random_seeds = np.random.choice(range(100), size = 3, replace=False).tolist()\n",
    "        val_df = pd.DataFrame()\n",
    "        test_df = pd.DataFrame()\n",
    "        print('Random seed: ', random_seeds)\n",
    "        for random_seed in random_seeds:\n",
    "            opt['random_seed'] = random_seed\n",
    "            model = basics.get_model(opt, wandb)\n",
    "            pred_df = train(model, opt)\n",
    "            val_df = pd.concat([val_df, pred_df])\n",
    "            \n",
    "            pred_df = model.test()\n",
    "            test_df = pd.concat([test_df, pred_df])\n",
    "            \n",
    "        stat_val = basics.avg_eval(val_df, opt, 'val')\n",
    "        stat_test = basics.avg_eval(test_df, opt, 'test')\n",
    "        model.log_wandb(stat_val.to_dict())\n",
    "        model.log_wandb(stat_test.to_dict())        \n",
    "    else:\n",
    "        \n",
    "        if opt['cross_testing']:\n",
    "            \n",
    "            test_df = pd.DataFrame()\n",
    "            method_model_path = opt['cross_testing_model_path']\n",
    "            model_paths = glob.glob(method_model_path + '/cross_domain_*.pth')\n",
    "            for model_path in model_paths:\n",
    "                opt['cross_testing_model_path_single'] = model_path\n",
    "                model = basics.get_model(opt, wandb)\n",
    "                pred_df = model.test()\n",
    "                \n",
    "                test_df = pd.concat([test_df, pred_df])\n",
    "            stat_test = basics.avg_eval(test_df, opt, 'cross_testing')\n",
    "            \n",
    "            model.log_wandb(stat_test.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28269cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab17045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
